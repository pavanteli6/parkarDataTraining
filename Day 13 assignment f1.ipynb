{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9dec2da-dd4a-46e8-97ae-5633fa26ee8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import input_file_name, current_timestamp\n",
    "import os\n",
    "\n",
    "# Volume path - âœ… Replace with your actual path\n",
    "VOLUME_PATH = \"/Volumes/newpavancatalog/bronze/f1files\"\n",
    "\n",
    "# Get all CSV files from volume\n",
    "csv_files = [f.path for f in dbutils.fs.ls(VOLUME_PATH) if f.name.endswith(\".csv\")]\n",
    "\n",
    "# Create a bronze table for each CSV file\n",
    "for csv_file in csv_files:\n",
    "    # Extract table name from file name\n",
    "    table_name = os.path.basename(csv_file).replace(\".csv\", \"\").replace(\"-\", \"_\").replace(\" \", \"_\").lower()\n",
    "    \n",
    "    @dlt.table(\n",
    "        name=f\"bronze_{table_name}\",\n",
    "        comment=f\"Bronze table for {os.path.basename(csv_file)}\",\n",
    "        table_properties={\"quality\": \"bronze\"}\n",
    "    )\n",
    "    def load_bronze_table(file_path=csv_file):\n",
    "        return (\n",
    "            spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .load(file_path)\n",
    "            .withColumn(\"ingestion_time\", current_timestamp())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b782cbe-d1aa-41a5-aa9a-69ce7174e4d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_circuits\",\n",
    "    comment=\"Cleaned and transformed payments data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def silver_payments():\n",
    "    df = dlt.read(\"bronze_circuits\")\n",
    "    # Example cleaning: trim strings, lowercase column names, remove nulls\n",
    "    cleaned_df = (\n",
    "        df.select([trim(lower(col(c))).alias(c) for c in df.columns])\n",
    "    )\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dfa087e-ac66-449c-b6b6-6ec7b43beee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_constructors\",\n",
    "    comment=\"Cleaned and transformed payments data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def silver_payments():\n",
    "    df = dlt.read(\"bronze_constructors\")\n",
    "    # Example cleaning: trim strings, lowercase column names, remove nulls\n",
    "    cleaned_df = (\n",
    "        df.select([trim(lower(col(c))).alias(c) for c in df.columns])\n",
    "    )\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24af3733-67e4-446b-9627-220901213d76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_drivers\",\n",
    "    comment=\"Cleaned and transformed payments data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def silver_payments():\n",
    "    df = dlt.read(\"bronze_drivers\")\n",
    "    # Example cleaning: trim strings, lowercase column names, remove nulls\n",
    "    cleaned_df = (\n",
    "        df.select([trim(lower(col(c))).alias(c) for c in df.columns])\n",
    "    )\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a216464-78df-484e-9cfc-5797f5a85079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_lap_times\",\n",
    "    comment=\"Cleaned and transformed payments data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def silver_payments():\n",
    "    df = dlt.read(\"bronze_lap_times\")\n",
    "    # Example cleaning: trim strings, lowercase column names, remove nulls\n",
    "    cleaned_df = (\n",
    "        df.select([trim(lower(col(c))).alias(c) for c in df.columns])\n",
    "    )\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "313dd40d-c22a-4a92-9208-d7180adf76b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_pit_stops\",\n",
    "    comment=\"Cleaned and transformed payments data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def silver_payments():\n",
    "    df = dlt.read(\"bronze_pit_stops\")\n",
    "    # Example cleaning: trim strings, lowercase column names, remove nulls\n",
    "    cleaned_df = (\n",
    "        df.select([trim(lower(col(c))).alias(c) for c in df.columns])\n",
    "    )\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2777714a-b50c-4b88-a8b5-0837a1a75a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_qualifying\",\n",
    "    comment=\"Cleaned and transformed payments data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def silver_payments():\n",
    "    df = dlt.read(\"bronze_qualifying\")\n",
    "    # Example cleaning: trim strings, lowercase column names, remove nulls\n",
    "    cleaned_df = (\n",
    "        df.select([trim(lower(col(c))).alias(c) for c in df.columns])\n",
    "    )\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3196bbf2-5580-4fb5-882a-38accd9e42d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_races\",\n",
    "    comment=\"Cleaned and transformed payments data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def silver_payments():\n",
    "    df = dlt.read(\"bronze_races\")\n",
    "    # Example cleaning: trim strings, lowercase column names, remove nulls\n",
    "    cleaned_df = (\n",
    "        df.select([trim(lower(col(c))).alias(c) for c in df.columns])\n",
    "    )\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d36e1a64-2ecb-4b21-824b-891c3be19d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_results\",\n",
    "    comment=\"Cleaned and transformed payments data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def silver_payments():\n",
    "    df = dlt.read(\"bronze_results\")\n",
    "    # Example cleaning: trim strings, lowercase column names, remove nulls\n",
    "    cleaned_df = (\n",
    "        df.select([trim(lower(col(c))).alias(c) for c in df.columns])\n",
    "    )\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dee0da1-c712-4125-a123-9d605fb77c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, count, avg, current_timestamp\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"gold_driver_summary\",\n",
    "    comment=\"Aggregated summary of driver information\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_driver_summary():\n",
    "    drivers_df = dlt.read(\"silver_drivers\")\n",
    "\n",
    "    # Example: Aggregate by nationality and year of birth\n",
    "    summary_df = (\n",
    "        drivers_df.groupBy(\"nationality\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"total_drivers\"),\n",
    "            avg(col(\"dob\").substr(1, 4).cast(\"int\")).alias(\"avg_birth_year\")\n",
    "        )\n",
    "        .withColumn(\"record_updated_time\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34d4b29c-4b84-4d18-9ae2-9ffeaddec88a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"gold_constructors_summary\",\n",
    "    comment=\"Aggregated stats for each F1 constructor (team) across all races\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_constructors_summary():\n",
    "    constructors_df = dlt.read(\"silver_constructors\")\n",
    "    results_df = dlt.read(\"silver_results\")\n",
    "    races_df = dlt.read(\"silver_races\")\n",
    "\n",
    "    # Join constructors with results and races\n",
    "    joined_df = (\n",
    "        results_df.join(constructors_df, \"constructorId\", \"left\")\n",
    "                  .join(races_df, \"raceId\", \"left\")\n",
    "    )\n",
    "\n",
    "    # Aggregate gold metrics, disambiguate columns\n",
    "    gold_df = (\n",
    "        joined_df.groupBy(\n",
    "            col(\"constructorId\"),\n",
    "            col(\"silver_constructors.name\"),\n",
    "            col(\"silver_constructors.nationality\")\n",
    "        )\n",
    "        .agg(\n",
    "            count(\"raceId\").alias(\"total_races\"),\n",
    "            count(col(\"position\")).alias(\"total_finishes\"),\n",
    "            count((col(\"position\") == 1).cast(\"int\")).alias(\"wins\")\n",
    "        )\n",
    "        .orderBy(col(\"wins\").desc())\n",
    "    )\n",
    "    \n",
    "    return gold_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 13 assignment f1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
